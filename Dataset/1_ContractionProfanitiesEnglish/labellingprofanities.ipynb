{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key features implemented:**  \n",
    "\n",
    "1. Phrase Extraction:  \n",
    "\n",
    "    a. Uses POS pattern grammar to detect noun/verb phrases  \n",
    "\n",
    "    b. Combines related words (e.g. \"personal attack\" → single phrase)  \n",
    "\n",
    "    c. Handles both single words and multi-word expressions  \n",
    "\n",
    "2. Semantic Analysis:  \n",
    "\n",
    "    a. Compares both full phrases and individual words  \n",
    "\n",
    "    b. Uses Wu-Palmer similarity for meaning comparison  \n",
    "\n",
    "    c. Considers different parts of speech for accurate matching  \n",
    "\n",
    "3. Context Handling:  \n",
    "\n",
    "    a. Prioritizes phrase-level analysis before word-level  \n",
    "\n",
    "    b. Uses POS filtering for better synset selection  \n",
    "\n",
    "    c. Implements similarity threshold (0.6) to reduce false positives  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, RegexpParser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "category_anchors = {\n",
    "    \"Insults & Personal Attacks\": [\n",
    "        'insult.n.01', 'stupidity.n.01', 'fool.n.01',\n",
    "        'contempt.n.01', 'mock.v.01'\n",
    "    ],\n",
    "    \"Hate Speech & Discrimination\": [\n",
    "        'prejudice.n.01', 'bigotry.n.01', 'intolerance.n.01',\n",
    "        'discrimination.n.01', 'racism.n.01'\n",
    "    ],\n",
    "    \"Threatening or Violent Language\": [\n",
    "        'threat.n.01', 'violence.n.01', 'kill.v.01',\n",
    "        'harm.v.01', 'attack.v.01'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase detection grammar\n",
    "grammar = r\"\"\"\n",
    "    NP: {<JJ>*<NN.*>+}  # Noun phrases\n",
    "    VP: {<VB.*><RB.*>?} # Verb phrases\n",
    "\"\"\"\n",
    "chunker = RegexpParser(grammar)\n",
    "\n",
    "def extract_phrases(text):\n",
    "    \"\"\"Extract meaningful phrases using POS tagging\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = chunker.parse(tagged)\n",
    "    \n",
    "    phrases = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() in ['NP', 'VP']:\n",
    "            phrases.append(' '.join(word for word, tag in subtree.leaves()))\n",
    "    return phrases or [text]\n",
    "\n",
    "def get_best_synset(word, pos=None):\n",
    "    \"\"\"Get most relevant synset with POS filtering\"\"\"\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if pos:\n",
    "        synsets = [s for s in synsets if s.pos() == pos]\n",
    "    return synsets[0] if synsets else None\n",
    "\n",
    "def semantic_similarity(phrase, category):\n",
    "    \"\"\"Calculate maximum similarity score between phrase and category\"\"\"\n",
    "    max_score = 0\n",
    "    words = word_tokenize(phrase)\n",
    "    \n",
    "    # Try full phrase first\n",
    "    phrase_syn = wordnet.synsets(phrase)\n",
    "    if phrase_syn:\n",
    "        for anchor in category_anchors[category]:\n",
    "            anchor_syn = wordnet.synset(anchor)\n",
    "            score = phrase_syn[0].wup_similarity(anchor_syn)\n",
    "            max_score = max(max_score, score or 0)\n",
    "    \n",
    "    # Check individual words\n",
    "    for word in words:\n",
    "        word_syn = get_best_synset(word)\n",
    "        if not word_syn:\n",
    "            continue\n",
    "            \n",
    "        for anchor in category_anchors[category]:\n",
    "            anchor_syn = wordnet.synset(anchor)\n",
    "            score = word_syn.wup_similarity(anchor_syn)\n",
    "            max_score = max(max_score, score or 0)\n",
    "    \n",
    "    return max_score\n",
    "\n",
    "def categorize_text(text):\n",
    "    phrases = extract_phrases(text)\n",
    "    scores = {cat: 0 for cat in category_anchors}\n",
    "    \n",
    "    for phrase in phrases:\n",
    "        for cat in category_anchors:\n",
    "            score = semantic_similarity(phrase.lower(), cat)\n",
    "            scores[cat] = max(scores[cat], score)\n",
    "    \n",
    "    max_score = max(scores.values())\n",
    "    if max_score < 0.6:\n",
    "        return \"Uncategorized\"\n",
    "    \n",
    "    return max(scores, key=scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "df = pd.read_csv(\"Profanities.csv\", encoding=\"latin1\", header=None, names=[\"Text\"])\n",
    "df[\"Category\"] = df[\"Text\"].apply(categorize_text)\n",
    "df.to_csv(\"semantically_labeled_profanities.csv\", index=False)\n",
    "\n",
    "print(\"✅ Categorization complete! Check 'semantically_labeled_profanities.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
