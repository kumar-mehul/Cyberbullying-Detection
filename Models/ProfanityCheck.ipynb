{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this code does?\n",
    "Hybrid Approach to Check if a sentance has profanity words in them. It uses both a CSV file and a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Step 1: Load the CSV file once and store words in a dictionary\n",
    "def load_profanity_words(csv_file):\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"latin1\")\n",
    "    profanity_dict = {row[\"Text\"].lower(): row[\"Category\"] for _, row in profanity_df.iterrows()}\n",
    "    return profanity_dict\n",
    "\n",
    "# Load the profanity dictionary (Executed once)\n",
    "profanity_dict = load_profanity_words(\"semantically_labeled_profanities.csv\")\n",
    "\n",
    "# Step 2: Function to check profanity in text\n",
    "def contains_profanity(text):\n",
    "    words = word_tokenize(text.lower())  # Tokenize sentence\n",
    "    flagged_words = {word: profanity_dict[word] for word in words if word in profanity_dict}  # Check words\n",
    "\n",
    "    return flagged_words if flagged_words else \"Clean\"  # Return flagged words with categories\n",
    "\n",
    "# Example usage\n",
    "text1 = \"You are an idiot and a loser!\"\n",
    "text2 = \"piece of shit\"\n",
    "\n",
    "print(contains_profanity(text1))  \n",
    "print(contains_profanity(text2))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Hybrid (CSV + Dictionary)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fast O(1) lookup (thanks to the dictionary)\n",
    "2. Easy to update (just modify CSV, no code change)\n",
    "3. Scalable \n",
    "\n",
    "For scaling further, using a databse or tree structure would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Refining by updating normalization part to handle elongated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-1.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer & Stemmerimport nltk\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer & Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load English dictionary from CSV file\n",
    "def load_dictionary(dictionary_path):\n",
    "    df = pd.read_csv(dictionary_path, header=None)  # Assuming no header\n",
    "    words = set(df[0].astype(str).str.lower())  # Convert words to lowercase for matching\n",
    "    return words\n",
    "\n",
    "# Provide the path to your dictionary.csv file\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "\n",
    "# Step 1: Reduce excessive repetition but keep meaningful words\n",
    "def reduce_redundant_letters(word):\n",
    "    \"\"\"\n",
    "    Dynamically reduces repeated letters while keeping valid words intact.\n",
    "    \"\"\"\n",
    "    # Reduce 3+ occurrences to 2 (e.g., \"loooove\" -> \"loove\")\n",
    "    reduced_word = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", word)\n",
    "\n",
    "    # Further reduce to a single letter if still not valid (e.g., \"happyyy\" -> \"happy\")\n",
    "    if reduced_word not in english_words:\n",
    "        reduced_word = re.sub(r\"(.)\\1+\", r\"\\1\", reduced_word)\n",
    "\n",
    "    return reduced_word\n",
    "\n",
    "# Step 2: Find closest match from dictionary\n",
    "def find_closest_match(word):\n",
    "    matches = difflib.get_close_matches(word, english_words, n=1, cutoff=0.8)  # Finds most similar word\n",
    "    return matches[0] if matches else word  # Return matched word or original if no match\n",
    "\n",
    "# Step 3: Normalize the word\n",
    "def normalize_word(word):\n",
    "    original_word = word.lower()\n",
    "\n",
    "    # Step 1: Reduce redundant letters\n",
    "    reduced_word = reduce_redundant_letters(original_word)\n",
    "\n",
    "    # Step 2: Remove non-alphabetic characters\n",
    "    cleaned_word = re.sub(r\"[^a-zA-Z]\", \"\", reduced_word)\n",
    "\n",
    "    # Step 3: Apply lemmatization and stemming\n",
    "    lemma = lemmatizer.lemmatize(cleaned_word)\n",
    "    stem = stemmer.stem(lemma)\n",
    "\n",
    "    # Step 4: Check with dictionary and find the best match\n",
    "    final_word = None\n",
    "    if cleaned_word in english_words:\n",
    "        final_word = cleaned_word\n",
    "    elif lemma in english_words:\n",
    "        final_word = lemma\n",
    "    elif stem in english_words:\n",
    "        final_word = stem\n",
    "    else:\n",
    "        final_word = find_closest_match(cleaned_word)  # Use fuzzy matching\n",
    "\n",
    "    print(f\"Original: {original_word}, Reduced: {reduced_word}, Cleaned: {cleaned_word}, Lemma: {lemma}, Stem: {stem}, Final: {final_word}\")\n",
    "\n",
    "    return final_word\n",
    "\n",
    "# Test Cases\n",
    "print(normalize_word(\"hiiiiiii\"))  \n",
    "print(normalize_word(\"beautifully\"))  \n",
    "print(normalize_word(\"loooooser\"))  \n",
    "print(normalize_word(\"hhhaappy\"))  \n",
    "print(normalize_word(\"shiiitttt\"))  \n",
    "print(normalize_word(\"badddd\"))  \n",
    "print(normalize_word(\"ssshuuttt\"))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load English dictionary from CSV file\n",
    "def load_dictionary(dictionary_path):\n",
    "    df = pd.read_csv(dictionary_path, header=None)  # Assuming no header\n",
    "    words = set(df[0].astype(str).str.lower())  # Convert words to lowercase for matching\n",
    "    return words\n",
    "\n",
    "# Provide the path to your dictionary.csv file\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "\n",
    "# Step 1: Reduce excessive repetition but keep meaningful words\n",
    "def reduce_redundant_letters(word):\n",
    "    \"\"\"\n",
    "    Dynamically reduces repeated letters while keeping valid words intact.\n",
    "    \"\"\"\n",
    "    # Reduce 3+ occurrences to 2 (e.g., \"loooove\" -> \"loove\")\n",
    "    reduced_word = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", word)\n",
    "\n",
    "    # Further reduce to a single letter if still not valid (e.g., \"happyyy\" -> \"happy\")\n",
    "    if reduced_word not in english_words:\n",
    "        reduced_word = re.sub(r\"(.)\\1+\", r\"\\1\", reduced_word)\n",
    "\n",
    "    return reduced_word\n",
    "\n",
    "# Step 2: Find closest match from dictionary\n",
    "def find_closest_match(word):\n",
    "    matches = difflib.get_close_matches(word, english_words, n=1, cutoff=0.8)  # Finds most similar word\n",
    "    return matches[0] if matches else word  # Return matched word or original if no match\n",
    "\n",
    "# Step 3: Normalize the word\n",
    "def normalize_word(word):\n",
    "    original_word = word.lower()\n",
    "\n",
    "    # Step 1: Reduce redundant letters\n",
    "    reduced_word = reduce_redundant_letters(original_word)\n",
    "\n",
    "    # Step 2: Remove non-alphabetic characters\n",
    "    cleaned_word = re.sub(r\"[^a-zA-Z]\", \"\", reduced_word)\n",
    "\n",
    "    # Step 3: Apply lemmatization and stemming\n",
    "    lemma = lemmatizer.lemmatize(cleaned_word)\n",
    "    stem = stemmer.stem(lemma)\n",
    "\n",
    "    # Step 4: Check with dictionary and find the best match\n",
    "    final_word = None\n",
    "    if cleaned_word in english_words:\n",
    "        final_word = cleaned_word\n",
    "    elif lemma in english_words:\n",
    "        final_word = lemma\n",
    "    elif stem in english_words:\n",
    "        final_word = stem\n",
    "    else:\n",
    "        final_word = find_closest_match(cleaned_word)  # Use fuzzy matching\n",
    "\n",
    "    print(f\"Original: {original_word}, Reduced: {reduced_word}, Cleaned: {cleaned_word}, Lemma: {lemma}, Stem: {stem}, Final: {final_word}\")\n",
    "\n",
    "    return final_word\n",
    "\n",
    "# Test Cases\n",
    "print(normalize_word(\"hiiiiiii\"))     \n",
    "print(normalize_word(\"beautifully\"))    \n",
    "print(normalize_word(\"loooooser\"))  \n",
    "print(normalize_word(\"hhhaappy\"))  \n",
    "print(normalize_word(\"shiiitttt\"))  \n",
    "print(normalize_word(\"badddd\"))  \n",
    "print(normalize_word(\"ssshuuttt\"))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Profanity Check which can handle elongated words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why This Works Well\n",
    "1. Text Normalization ensures words like \"loooooser\" → \"loser\". It cleans, lemmatizes, stems, and corrects misspelled/elongated words. Hence handles excessive letter repetition and typos.  \n",
    "2. Uses a preloaded dictionary to detect and categorize profane words. Looks up words in the dictionary, falling back on fuzzy matching if needed for better accuracy. \n",
    "3. Loads CSVs once to avoid redundant I/O operations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize Lemmatizer & Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load English Dictionary\n",
    "def load_dictionary(dictionary_path):\n",
    "    df = pd.read_csv(dictionary_path, header=None)  # Assuming no header\n",
    "    words = set(df[0].astype(str).str.lower())  # Convert words to lowercase for matching\n",
    "    return words\n",
    "\n",
    "# Load Profanity Dictionary\n",
    "def load_profanity_words(csv_file):\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"latin1\")\n",
    "    profanity_dict = {row[\"Text\"].lower(): row[\"Category\"] for _, row in profanity_df.iterrows()}\n",
    "    return profanity_dict\n",
    "\n",
    "# Load dictionaries (Executed once)\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "profanity_path = \"semantically_labeled_profanities.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "profanity_dict = load_profanity_words(profanity_path)\n",
    "\n",
    "# Reduce excessive repetition in words\n",
    "def reduce_redundant_letters(word):\n",
    "    reduced_word = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", word)  # Limit 3+ to 2\n",
    "    if reduced_word not in english_words:\n",
    "        reduced_word = re.sub(r\"(.)\\1+\", r\"\\1\", reduced_word)  # Further reduce to 1 if needed\n",
    "    return reduced_word\n",
    "\n",
    "# Find closest dictionary match\n",
    "def find_closest_match(word):\n",
    "    matches = difflib.get_close_matches(word, english_words, n=1, cutoff=0.8)\n",
    "    return matches[0] if matches else word\n",
    "\n",
    "# Normalize the word\n",
    "def normalize_word(word):\n",
    "    original_word = word.lower()\n",
    "    reduced_word = reduce_redundant_letters(original_word)\n",
    "    cleaned_word = re.sub(r\"[^a-zA-Z]\", \"\", reduced_word)\n",
    "\n",
    "    lemma = lemmatizer.lemmatize(cleaned_word)\n",
    "    stem = stemmer.stem(lemma)\n",
    "\n",
    "    final_word = (\n",
    "        cleaned_word if cleaned_word in english_words else\n",
    "        lemma if lemma in english_words else\n",
    "        stem if stem in english_words else\n",
    "        find_closest_match(cleaned_word)\n",
    "    )\n",
    "\n",
    "    return final_word\n",
    "\n",
    "# Check if text contains profanity and only return flagged words\n",
    "def contains_profanity(text):\n",
    "    words = word_tokenize(text.lower())  # Tokenize text\n",
    "    normalized_words = [normalize_word(word) for word in words]  # Normalize words\n",
    "\n",
    "    flagged_words = {word: profanity_dict[word] for word in normalized_words if word in profanity_dict}\n",
    "    \n",
    "    return flagged_words if flagged_words else \"Clean\"\n",
    "\n",
    "# Example Usage\n",
    "text1 = \"You are an idiot and a loooooser!\"\n",
    "text2 = \"piece of shiiitttt\"\n",
    "\n",
    "print(contains_profanity(text1))  \n",
    "print(contains_profanity(text2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving model to handle symbol-based Obfuscations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from rapidfuzz import process\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize Lemmatizer & Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load English Dictionary\n",
    "def load_dictionary(dictionary_path):\n",
    "    df = pd.read_csv(dictionary_path, header=None)  # Assuming no header\n",
    "    words = set(df[0].astype(str).str.lower())  # Convert words to lowercase for matching\n",
    "    return words\n",
    "\n",
    "# Load Profanity Dictionary\n",
    "def load_profanity_words(csv_file):\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"utf-8\")  # Force UTF-8 encoding\n",
    "    profanity_df[\"Text\"] = profanity_df[\"Text\"].str.replace(r\"[\\x92\\x91\\x96\\x97]\", \"'\", regex=True) \n",
    "    profanity_dict = {row[\"Text\"].lower(): row[\"Category\"] for _, row in profanity_df.iterrows()}\n",
    "    return profanity_dict\n",
    "\n",
    "# Load dictionaries (Executed once)\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "profanity_path = \"semantically_labeled_profanities.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "profanity_dict = load_profanity_words(profanity_path)\n",
    "\n",
    "# Symbol based obfuscations\n",
    "def deobfuscate_word(word):\n",
    "    \"\"\" Convert common leetspeak & symbol obfuscations back to letters \"\"\"\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[@]', 'a', word)\n",
    "    word = re.sub(r'[$]', 's', word)\n",
    "    word = re.sub(r'[1!]', 'i', word)\n",
    "    word = re.sub(r'[0]', 'o', word)\n",
    "    word = re.sub(r'[3]', 'e', word)\n",
    "    word = re.sub(r'[4]', 'a', word)\n",
    "    word = re.sub(r'[5]', 's', word)\n",
    "    word = re.sub(r'[7]', 't', word)\n",
    "    word = re.sub(r'[8]', 'b', word)\n",
    "    word = re.sub(r'[9]', 'g', word)\n",
    "    return word\n",
    "\n",
    "\n",
    "# Fuzzy matching using rapidfuzz \n",
    "def find_closest_profanity(word):\n",
    "    match = process.extractOne(word, profanity_dict.keys(), score_cutoff=85)  # Increased cutoff\n",
    "    return match[0] if match else None  # Return None if no good match\n",
    "\n",
    "# Reduce excessive repetition in words\n",
    "def reduce_redundant_letters(word):\n",
    "    reduced_word = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", word)  # Limit 3+ to 2\n",
    "    if reduced_word not in english_words:\n",
    "        reduced_word = re.sub(r\"(.)\\1+\", r\"\\1\", reduced_word)  # Further reduce to 1 if needed\n",
    "    return reduced_word\n",
    "\n",
    "# Find closest dictionary match\n",
    "def find_closest_match(word):\n",
    "    matches = difflib.get_close_matches(word, english_words, n=1, cutoff=0.8)\n",
    "    return matches[0] if matches else word\n",
    "\n",
    "# Normalize the word\n",
    "def normalize_word(word):\n",
    "    # Skip very short words (they cause false positives)\n",
    "    if len(word) <= 2:\n",
    "        return word\n",
    "        \n",
    "    original_word = word.lower()\n",
    "    deobfuscated_word = deobfuscate_word(original_word)\n",
    "    reduced_word = reduce_redundant_letters(deobfuscated_word)\n",
    "    cleaned_word = re.sub(r\"[^a-zA-Z]\", \"\", reduced_word)  # Remove ALL non-alphabetic\n",
    "    \n",
    "    # Skip normalization if the original is a known English word\n",
    "    if cleaned_word in english_words:\n",
    "        return cleaned_word\n",
    "        \n",
    "    # Only proceed with lemmatization/stemming if necessary\n",
    "    if cleaned_word not in profanity_dict:\n",
    "        lemma = lemmatizer.lemmatize(cleaned_word)\n",
    "        if lemma in profanity_dict:\n",
    "            return lemma\n",
    "        stem = stemmer.stem(cleaned_word)\n",
    "        if stem in profanity_dict:\n",
    "            return stem\n",
    "    \n",
    "    return cleaned_word\n",
    "\n",
    "# Check if text contains profanity and only return flagged words\n",
    "def contains_profanity(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    flagged_words = {}\n",
    "    \n",
    "    for word in words:\n",
    "        normalized = normalize_word(word)\n",
    "        \n",
    "        # Direct match\n",
    "        if normalized in profanity_dict:\n",
    "            flagged_words[word] = profanity_dict[normalized]  # Keep original word\n",
    "            continue\n",
    "            \n",
    "        # Fuzzy match only if word is suspiciously similar to profanity\n",
    "        if len(normalized) > 4:  # Only check longer words\n",
    "            closest = find_closest_profanity(normalized)\n",
    "            if closest:\n",
    "                flagged_words[word] = profanity_dict[closest]\n",
    "    \n",
    "    return flagged_words or \"Clean\"\n",
    "\n",
    "# Example Usage\n",
    "text1 = \"You are a f@cking id!0t\"\n",
    "text2 = \"h3ll yeah! that's $tupid\"\n",
    "text3 = \"Sh1t, you're an @ssh0le!\"\n",
    "\n",
    "\n",
    "print(contains_profanity(text1))  \n",
    "print(contains_profanity(text2))\n",
    "print(contains_profanity(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output of V-3:  \n",
    "{'cking': 'Vulgar'}  \n",
    "{'that': 'Insults & Personal Attacks', 'tupid': 'Insults & Personal Attacks'}  \n",
    "{'Sh1t': 'Insults & Personal Attacks', 'ssh0le': 'Explicit'}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from rapidfuzz import process\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#English stop words\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize Lemmatizer & Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load English Dictionary\n",
    "def load_dictionary(dictionary_path):\n",
    "    df = pd.read_csv(dictionary_path, header=None)  # Assuming no header\n",
    "    words = set(df[0].astype(str).str.lower())  # Convert words to lowercase for matching\n",
    "    return words\n",
    "\n",
    "# Load Profanity Dictionary\n",
    "def load_profanity_words(csv_file):\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"utf-8\")  # Force UTF-8 encoding\n",
    "    profanity_df[\"Text\"] = profanity_df[\"Text\"].str.replace(r\"[\\x92\\x91\\x96\\x97]\", \"'\", regex=True)\n",
    "    profanity_dict = {row[\"Text\"].lower(): row[\"Category\"] for _, row in profanity_df.iterrows()}\n",
    "    return profanity_dict\n",
    "\n",
    "# Load dictionaries (Executed once)\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "profanity_path = \"semantically_labeled_profanities.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "profanity_dict = load_profanity_words(profanity_path)\n",
    "\n",
    "# Symbol based obfuscations\n",
    "def deobfuscate_word(word):\n",
    "    \"\"\"More comprehensive leetspeak handling with priority for profanity patterns\"\"\"\n",
    "    # First handle common profanity obfuscations\n",
    "    profanity_patterns = {\n",
    "        r'f[!@#$%^&*.]*u[!@#$%^&*.]*c[!@#$%^&*.]*k': 'fuck',\n",
    "        r's[!@#$%^&*.]*h[!@#$%^&*.]*i[!@#$%^&*.]*t': 'shit',\n",
    "        r'a[!@#$%^&*.]*s[!@#$%^&*.]*s': 'ass',\n",
    "        r'b[!@#$%^&*.]*i[!@#$%^&*.]*t[!@#$%^&*.]*c[!@#$%^&*.]*h': 'bitch',\n",
    "        r'c[!@#$%^&*.]*u[!@#$%^&*.]*n[!@#$%^&*.]*t': 'cunt',\n",
    "        r'd[!@#$%^&*.]*i[!@#$%^&*.]*c[!@#$%^&*.]*k': 'dick',\n",
    "        r'p[!@#$%^&*.]*u[!@#$%^&*.]*s[!@#$%^&*.]*s[!@#$%^&*.]*y': 'pussy',\n",
    "    }\n",
    "\n",
    "    lower_word = word.lower()\n",
    "    for pattern, replacement in profanity_patterns.items():\n",
    "        if re.fullmatch(pattern, lower_word):\n",
    "            return replacement\n",
    "\n",
    "    # Then handle general leetspeak\n",
    "    replacements = {\n",
    "        r'[@4]': 'a',\n",
    "        r'[$5]': 's',\n",
    "        r'[1!|i]': 'i',\n",
    "        r'[0°]': 'o',\n",
    "        r'[3]': 'e',\n",
    "        r'[7]': 't',\n",
    "        r'[8]': 'b',\n",
    "        r'[9]': 'g',\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        lower_word = re.sub(pattern, replacement, lower_word)\n",
    "\n",
    "    return lower_word\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    # Handle contractions and apostrophes properly\n",
    "    text = re.sub(r\"(\\w+)'(\\w+)\", r\"\\1'\\2\", text)  # Keep apostrophes within words\n",
    "    # Tokenize while preserving obfuscated words\n",
    "    tokens = re.findall(r\"[@\\w$!]+\", text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Fuzzy matching using rapidfuzz\n",
    "def find_closest_profanity(word):\n",
    "    match = process.extractOne(word, profanity_dict.keys(), score_cutoff=85)  # Increased cutoff\n",
    "    return match[0] if match else None  # Return None if no good match\n",
    "\n",
    "# Reduce excessive repetition in words\n",
    "def reduce_redundant_letters(word):\n",
    "    \"\"\"More conservative letter reduction\"\"\"\n",
    "    # First pass: Reduce 3+ repeats to 2 (e.g., \"shittt\" → \"shitt\")\n",
    "    reduced = re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "    # Only reduce to single letter if the word isn't in dictionary\n",
    "    if reduced not in english_words:\n",
    "        reduced = re.sub(r'(.)\\1+', r'\\1', reduced)\n",
    "    return reduced\n",
    "\n",
    "# Find closest dictionary match\n",
    "def find_closest_match(word):\n",
    "    matches = difflib.get_close_matches(word, english_words, n=1, cutoff=0.8)\n",
    "    return matches[0] if matches else word\n",
    "\n",
    "# Normalize the word\n",
    "def normalize_word(word):\n",
    "    if word in STOP_WORDS:\n",
    "        return word\n",
    "    \"\"\"Better handling of edge cases with profanity priority\"\"\"\n",
    "    if len(word) <= 2:  # Skip very short words\n",
    "        return word\n",
    "\n",
    "    # Step 1: Deobfuscate (convert numbers/symbols to letters)\n",
    "    deobfuscated = deobfuscate_word(word)\n",
    "\n",
    "    # Step 2: Handle common profanity fragments\n",
    "    if deobfuscated.endswith(('ing', 'in', 'ed')):\n",
    "        base_form = deobfuscated.rstrip('ing').rstrip('in').rstrip('ed')\n",
    "        if base_form in profanity_dict:\n",
    "            return base_form\n",
    "\n",
    "    # Step 3: Clean special characters but preserve intentional obfuscation\n",
    "    cleaned = re.sub(r\"[^a-z]\", \"\", deobfuscated)\n",
    "\n",
    "    # Step 4: Check for direct match in profanity dictionary\n",
    "    if cleaned in profanity_dict:\n",
    "        return cleaned\n",
    "\n",
    "    # Step 5: Check for close matches in the profanity dictionary\n",
    "    closest = process.extractOne(cleaned, profanity_dict.keys(), score_cutoff=90)\n",
    "    if closest:\n",
    "        return closest[0]\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# Check if text contains profanity and only return flagged words\n",
    "def contains_profanity(text):\n",
    "    words = custom_tokenize(text)\n",
    "    flagged_words = {}\n",
    "\n",
    "    for original_word in words:\n",
    "        # Skip stop words entirely\n",
    "        if original_word in STOP_WORDS:\n",
    "            continue\n",
    "\n",
    "        normalized = normalize_word(original_word)\n",
    "\n",
    "        # Additional check - skip if normalized became a stop word\n",
    "        if normalized in STOP_WORDS:\n",
    "            continue\n",
    "\n",
    "        # Only check against profanity dictionary\n",
    "        if normalized in profanity_dict:\n",
    "            flagged_words[original_word] = profanity_dict[normalized]\n",
    "\n",
    "    return flagged_words if flagged_words else \"Clean\"\n",
    "\n",
    "# Example Usage\n",
    "text1 = \"You are a f@cking id!0t\"\n",
    "text2 = \"h3ll yeah! that's $tupid\"\n",
    "text3 = \"Sh1t, you're an @ssh0le!\"\n",
    "\n",
    "\n",
    "print(contains_profanity(text1))\n",
    "print(contains_profanity(text2))\n",
    "print(contains_profanity(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output of V-4:  \n",
    "{'id!0t': 'Insults & Personal Attacks'}  \n",
    "{'$tupid': 'Insults & Personal Attacks'}  \n",
    "{'sh1t': 'Insults & Personal Attacks', '@ssh0le!': 'Explicit'}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-5**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from rapidfuzz import process\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLP tools\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load dictionaries\n",
    "def load_dictionary(dictionary_path):\n",
    "    \"\"\"Load English dictionary words\"\"\"\n",
    "    df = pd.read_csv(dictionary_path, header=None)\n",
    "    return set(df[0].astype(str).str.lower())\n",
    "\n",
    "def load_profanity_words(csv_file):\n",
    "    \"\"\"Load and preprocess profanity dictionary\"\"\"\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"latin-1\")\n",
    "    profanity_df[\"Text\"] = profanity_df[\"Text\"].str.replace(r\"[\\x92\\x91\\x96\\x97]\", \"'\", regex=True)\n",
    "\n",
    "    # Create both single-word and phrase dictionaries\n",
    "    single_words = {}\n",
    "    phrases = defaultdict(list)\n",
    "\n",
    "    for _, row in profanity_df.iterrows():\n",
    "        text = row[\"Text\"].lower()\n",
    "        if ' ' in text:  # It's a phrase\n",
    "            phrases[len(text.split())].append((text, row[\"Category\"]))\n",
    "        else:\n",
    "            single_words[text] = row[\"Category\"]\n",
    "\n",
    "    return single_words, phrases\n",
    "\n",
    "# Load dictionaries\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "profanity_path = \"semantically_labeled_profanities.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "profanity_dict, profanity_phrases = load_profanity_words(profanity_path)\n",
    "\n",
    "def deobfuscate_word(word):\n",
    "    \"\"\"Convert leetspeak and symbols back to letters with profanity priority\"\"\"\n",
    "    # Common profanity patterns\n",
    "    profanity_patterns = {\n",
    "        r'f[!@#$%^&*.]*u[!@#$%^&*.]*c[!@#$%^&*.]*k': 'fuck',\n",
    "        r's[!@#$%^&*.]*h[!@#$%^&*.]*i[!@#$%^&*.]*t': 'shit',\n",
    "        r'a[!@#$%^&*.]*s[!@#$%^&*.]*s': 'ass',\n",
    "        r'b[!@#$%^&*.]*i[!@#$%^&*.]*t[!@#$%^&*.]*c[!@#$%^&*.]*h': 'bitch',\n",
    "        r'c[!@#$%^&*.]*u[!@#$%^&*.]*n[!@#$%^&*.]*t': 'cunt',\n",
    "        r'd[!@#$%^&*.]*i[!@#$%^&*.]*c[!@#$%^&*.]*k': 'dick',\n",
    "        r'p[!@#$%^&*.]*u[!@#$%^&*.]*s[!@#$%^&*.]*s[!@#$%^&*.]*y': 'pussy',\n",
    "    }\n",
    "\n",
    "    lower_word = word.lower()\n",
    "    for pattern, replacement in profanity_patterns.items():\n",
    "        if re.fullmatch(pattern, lower_word):\n",
    "            return replacement\n",
    "\n",
    "    # General leetspeak replacements\n",
    "    replacements = {\n",
    "        r'[@4]': 'a',\n",
    "        r'[$5]': 's',\n",
    "        r'[1!|i]': 'i',\n",
    "        r'[0°]': 'o',\n",
    "        r'[3]': 'e',\n",
    "        r'[7]': 't',\n",
    "        r'[8]': 'b',\n",
    "        r'[9]': 'g',\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        lower_word = re.sub(pattern, replacement, lower_word)\n",
    "\n",
    "    return lower_word\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    \"\"\"Improved tokenizer that handles contractions and obfuscations\"\"\"\n",
    "    # Handle contractions\n",
    "    text = re.sub(r\"(\\w+)'(\\w+)\", r\"\\1'\\2\", text)\n",
    "    # Tokenize while preserving obfuscated words\n",
    "    return re.findall(r\"[@\\w$!']+\", text.lower())\n",
    "\n",
    "def reduce_redundant_letters(word):\n",
    "    \"\"\"Handle excessive letter repetition\"\"\"\n",
    "    # First reduce 3+ repeats to 2\n",
    "    reduced = re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "    # Only reduce further if needed\n",
    "    if reduced not in english_words:\n",
    "        reduced = re.sub(r'(.)\\1+', r'\\1', reduced)\n",
    "    return reduced\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"Normalize word with profanity detection focus\"\"\"\n",
    "    if len(word) <= 2 or word in STOP_WORDS:\n",
    "        return word\n",
    "\n",
    "    # Deobfuscate first\n",
    "    deobfuscated = deobfuscate_word(word)\n",
    "\n",
    "    # Handle common profanity fragments\n",
    "    if deobfuscated.endswith(('ing', 'in', 'ed')):\n",
    "        base_form = deobfuscated.rstrip('ing').rstrip('in').rstrip('ed')\n",
    "        if base_form in profanity_dict:\n",
    "            return base_form\n",
    "\n",
    "    # Clean and check\n",
    "    cleaned = re.sub(r\"[^a-z]\", \"\", deobfuscated)\n",
    "    if cleaned in profanity_dict:\n",
    "        return cleaned\n",
    "\n",
    "    # Fuzzy match only if word looks suspicious\n",
    "    if len(cleaned) > 4:\n",
    "        closest = process.extractOne(cleaned, profanity_dict.keys(), score_cutoff=90)\n",
    "        if closest:\n",
    "            return closest[0]\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "def contains_profanity(text):\n",
    "    \"\"\"Main profanity detection function with phrase support\"\"\"\n",
    "    words = custom_tokenize(text)\n",
    "    flagged_words = {}\n",
    "\n",
    "    # Check single words\n",
    "    for i, word in enumerate(words):\n",
    "        if word in STOP_WORDS or len(word) <= 2:\n",
    "            continue\n",
    "\n",
    "        normalized = normalize_word(word)\n",
    "        if normalized in STOP_WORDS:\n",
    "            continue\n",
    "\n",
    "        if normalized in profanity_dict:\n",
    "            flagged_words[word] = profanity_dict[normalized]\n",
    "\n",
    "    # Check multi-word phrases (2-4 words)\n",
    "    for phrase_length in range(2, min(5, len(words) + 1)): #Corrected the syntax error here\n",
    "        for i in range(len(words) - phrase_length + 1):\n",
    "            phrase = ' '.join(words[i:i+phrase_length])\n",
    "            #The following if statement and for loop were outside the contains_profanity function, they have been correctly indented.\n",
    "            if phrase in profanity_phrases[phrase_length]:\n",
    "                for term, category in profanity_phrases[phrase_length]:\n",
    "                    if term == phrase:\n",
    "                        flagged_words[phrase] = category\n",
    "                        break\n",
    "\n",
    "    return flagged_words if flagged_words else \"Clean\"\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_cases = [\n",
    "        \"You are a f@cking id!0t\",\n",
    "        \"h3ll yeah! that's $tupid\",\n",
    "        \"Sh1t, you're an @ssh0le!\",\n",
    "        \"This is a clean sentence\",\n",
    "        \"Go to hell you motherfucker\",\n",
    "        \"That's some bullshit right there\"\n",
    "    ]\n",
    "\n",
    "    for text in test_cases:\n",
    "        print(f\"Text: {text}\")\n",
    "        result = contains_profanity(text)\n",
    "        print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of V5:  \n",
    "\n",
    "Text: You are a f@cking id!0t  \n",
    "Result: {'id!0t': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: h3ll yeah! that's $tupid  \n",
    "Result: {'$tupid': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: Sh1t, you're an @ssh0le!  \n",
    "Result: {'sh1t': 'Insults & Personal Attacks', '@ssh0le!': 'Explicit'}  \n",
    "\n",
    "Text: This is a clean sentence  \n",
    "Result: Clean  \n",
    "\n",
    "Text: Go to hell you motherfucker  \n",
    "Result: {'motherfucker': 'Explicit'}  \n",
    "\n",
    "Text: That's some bullshit right there  \n",
    "Result: {'bullshit': 'Vulgar'}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-6**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from rapidfuzz import process\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize NLP tools\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load dictionaries\n",
    "def load_dictionary(dictionary_path):\n",
    "    \"\"\"Load English dictionary words\"\"\"\n",
    "    df = pd.read_csv(dictionary_path, header=None)\n",
    "    return set(df[0].astype(str).str.lower())\n",
    "\n",
    "def load_profanity_words(csv_file):\n",
    "    \"\"\"Load and preprocess profanity dictionary\"\"\"\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"latin-1\")\n",
    "    profanity_df[\"Text\"] = profanity_df[\"Text\"].str.replace(r\"[\\x92\\x91\\x96\\x97]\", \"'\", regex=True)\n",
    "\n",
    "    # Create both single-word and phrase dictionaries\n",
    "    single_words = {}\n",
    "    phrases = defaultdict(list)\n",
    "\n",
    "    for _, row in profanity_df.iterrows():\n",
    "        text = row[\"Text\"].lower()\n",
    "        if ' ' in text:  # It's a phrase\n",
    "            phrases[len(text.split())].append((text, row[\"Category\"]))\n",
    "        else:\n",
    "            single_words[text] = row[\"Category\"]\n",
    "\n",
    "    return single_words, phrases\n",
    "\n",
    "# Load dictionaries\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "profanity_path = \"semantically_labeled_profanities.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "profanity_dict, profanity_phrases = load_profanity_words(profanity_path)\n",
    "\n",
    "def deobfuscate_word(word):\n",
    "    \"\"\"Convert leetspeak and symbols back to letters with profanity priority\"\"\"\n",
    "    # Common profanity patterns\n",
    "    profanity_patterns = {\n",
    "        r'f[!@#$%^&*.]*u[!@#$%^&*.]*c[!@#$%^&*.]*k': 'fuck',\n",
    "        r's[!@#$%^&*.]*h[!@#$%^&*.]*i[!@#$%^&*.]*t': 'shit',\n",
    "        r'a[!@#$%^&*.]*s[!@#$%^&*.]*s': 'ass',\n",
    "        r'b[!@#$%^&*.]*i[!@#$%^&*.]*t[!@#$%^&*.]*c[!@#$%^&*.]*h': 'bitch',\n",
    "        r'c[!@#$%^&*.]*u[!@#$%^&*.]*n[!@#$%^&*.]*t': 'cunt',\n",
    "        r'd[!@#$%^&*.]*i[!@#$%^&*.]*c[!@#$%^&*.]*k': 'dick',\n",
    "        r'p[!@#$%^&*.]*u[!@#$%^&*.]*s[!@#$%^&*.]*s[!@#$%^&*.]*y': 'pussy',\n",
    "    }\n",
    "\n",
    "    lower_word = word.lower()\n",
    "    for pattern, replacement in profanity_patterns.items():\n",
    "        if re.fullmatch(pattern, lower_word):\n",
    "            return replacement\n",
    "\n",
    "    # General leetspeak replacements\n",
    "    replacements = {\n",
    "        r'[@4]': 'a',\n",
    "        r'[$5]': 's',\n",
    "        r'[1!|i]': 'i',\n",
    "        r'[0°]': 'o',\n",
    "        r'[3]': 'e',\n",
    "        r'[7]': 't',\n",
    "        r'[8]': 'b',\n",
    "        r'[9]': 'g',\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        lower_word = re.sub(pattern, replacement, lower_word)\n",
    "\n",
    "    return lower_word\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    \"\"\"Improved tokenizer that handles contractions and obfuscations\"\"\"\n",
    "    # Handle contractions\n",
    "    text = re.sub(r\"(\\w+)'(\\w+)\", r\"\\1'\\2\", text)\n",
    "    # Tokenize while preserving obfuscated words\n",
    "    return re.findall(r\"[@\\w$!']+\", text.lower())\n",
    "\n",
    "def reduce_redundant_letters(word):\n",
    "    \"\"\"Handle excessive letter repetition\"\"\"\n",
    "    # First reduce 3+ repeats to 2\n",
    "    reduced = re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "    # Only reduce further if needed\n",
    "    if reduced not in english_words:\n",
    "        reduced = re.sub(r'(.)\\1+', r'\\1', reduced)\n",
    "    return reduced\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"Normalize word with profanity detection focus\"\"\"\n",
    "    if len(word) <= 2 or word in STOP_WORDS:\n",
    "        return word\n",
    "\n",
    "    # Deobfuscate first\n",
    "    deobfuscated = deobfuscate_word(word)\n",
    "\n",
    "    # Handle common suffixes\n",
    "    if deobfuscated.endswith('ing'):\n",
    "        base_form = deobfuscated[:-3]\n",
    "        if base_form in profanity_dict:\n",
    "            return base_form\n",
    "        if f\"{base_form}in\" in profanity_dict:  # For words like \"fuckin\"\n",
    "            return f\"{base_form}in\"\n",
    "\n",
    "    # Check direct match\n",
    "    if deobfuscated in profanity_dict:\n",
    "        return deobfuscated\n",
    "\n",
    "    # Try fuzzy matching for close variants\n",
    "    closest = process.extractOne(deobfuscated, profanity_dict.keys(), score_cutoff=85)\n",
    "    if closest:\n",
    "        return closest[0]\n",
    "\n",
    "    return deobfuscated\n",
    "\n",
    "def contains_profanity(text):\n",
    "    \"\"\"Main profanity detection function with phrase support\"\"\"\n",
    "    words = custom_tokenize(text)\n",
    "    flagged_words = {}\n",
    "\n",
    "    # Check single words\n",
    "    for i, word in enumerate(words):\n",
    "        if word in STOP_WORDS or len(word) <= 2:\n",
    "            continue\n",
    "\n",
    "        normalized = normalize_word(word)\n",
    "        if normalized in STOP_WORDS:\n",
    "            continue\n",
    "\n",
    "        if normalized in profanity_dict:\n",
    "            flagged_words[word] = profanity_dict[normalized]\n",
    "\n",
    "    # Check multi-word phrases (2-4 words)\n",
    "    for phrase_length in range(2, min(5, len(words) + 1)): #Corrected the syntax error here\n",
    "        for i in range(len(words) - phrase_length + 1):\n",
    "            phrase = ' '.join(words[i:i+phrase_length])\n",
    "            #The following if statement and for loop were outside the contains_profanity function, they have been correctly indented.\n",
    "            if phrase in profanity_phrases[phrase_length]:\n",
    "                for term, category in profanity_phrases[phrase_length]:\n",
    "                    if term == phrase:\n",
    "                        flagged_words[phrase] = category\n",
    "                        break\n",
    "\n",
    "    return flagged_words if flagged_words else \"Clean\"\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test cases\n",
    "    test_cases = [\n",
    "      \"You are a f@cking id!0t\",\n",
    "      \"h3ll yeah! that's $tupid\",\n",
    "      \"Sh1t, you're an @ssh0le!\",\n",
    "      \"This is a clean sentence\",\n",
    "      \"Go to hell you motherfucker\",\n",
    "      \"That's some bullshit right there\",\n",
    "      \"What the f*** is this?\",\n",
    "      \"You're a dumb@ss\"\n",
    "    ]\n",
    "\n",
    "    for text in test_cases:\n",
    "      print(f\"Text: {text}\")\n",
    "      result = contains_profanity(text)\n",
    "      print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model Handling most of the sentences:  \n",
    "\n",
    "Text: You are a f@cking id!0t  \n",
    "Result: {'f@cking': 'Explicit', 'id!0t': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: h3ll yeah! that's $tupid  \n",
    "Result: {'$tupid': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: Sh1t, you're an @ssh0le!  \n",
    "Result: {'sh1t': 'Insults & Personal Attacks', '@ssh0le!': 'Explicit'}  \n",
    "\n",
    "Text: This is a clean sentence  \n",
    "Result: Clean  \n",
    "\n",
    "Text: Go to hell you motherfucker  \n",
    "Result: {'motherfucker': 'Explicit'}  \n",
    "\n",
    "Text: That's some bullshit right there  \n",
    "Result: {'bullshit': 'Vulgar'}  \n",
    "\n",
    "Text: What the f*** is this?  \n",
    "Result: Clean  \n",
    "\n",
    "Text: You're a dumb@ss  \n",
    "Result: {'dumb@ss': 'Insults & Personal Attacks'}  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
