{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context awareness for avaoiding false positives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-1**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from rapidfuzz import process\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize NLP tools\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load dictionaries\n",
    "def load_dictionary(dictionary_path):\n",
    "    \"\"\"Load English dictionary words\"\"\"\n",
    "    df = pd.read_csv(dictionary_path, header=None)\n",
    "    return set(df[0].astype(str).str.lower())\n",
    "\n",
    "def load_profanity_words(csv_file):\n",
    "    \"\"\"Load and preprocess profanity dictionary\"\"\"\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"latin-1\")\n",
    "    profanity_df[\"Text\"] = profanity_df[\"Text\"].str.replace(r\"[\\x92\\x91\\x96\\x97]\", \"'\", regex=True)\n",
    "    single_words = {}\n",
    "    phrases = defaultdict(list)\n",
    "    for _, row in profanity_df.iterrows():\n",
    "        text = row[\"Text\"].lower()\n",
    "        if ' ' in text:  # It's a phrase\n",
    "            phrases[len(text.split())].append((text, row[\"Category\"]))\n",
    "        else:\n",
    "            single_words[text] = row[\"Category\"]\n",
    "    return single_words, phrases\n",
    "\n",
    "# Load dictionaries\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "profanity_path = \"semantically_labeled_profanities.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "profanity_dict, profanity_phrases = load_profanity_words(profanity_path)\n",
    "\n",
    "def deobfuscate_word(word):\n",
    "    \"\"\"Convert leetspeak and symbols back to letters with profanity priority\"\"\"\n",
    "    profanity_patterns = {\n",
    "        r'f[!@#$%^&*.]*u[!@#$%^&*.]*c[!@#$%^&*.]*k': 'fuck',\n",
    "        r's[!@#$%^&*.]*h[!@#$%^&*.]*i[!@#$%^&*.]*t': 'shit',\n",
    "        r'a[!@#$%^&*.]*s[!@#$%^&*.]*s': 'ass',\n",
    "        r'b[!@#$%^&*.]*i[!@#$%^&*.]*t[!@#$%^&*.]*c[!@#$%^&*.]*h': 'bitch',\n",
    "        r'c[!@#$%^&*.]*u[!@#$%^&*.]*n[!@#$%^&*.]*t': 'cunt',\n",
    "        r'd[!@#$%^&*.]*i[!@#$%^&*.]*c[!@#$%^&*.]*k': 'dick',\n",
    "        r'p[!@#$%^&*.]*u[!@#$%^&*.]*s[!@#$%^&*.]*s[!@#$%^&*.]*y': 'pussy',\n",
    "    }\n",
    "    lower_word = word.lower()\n",
    "    for pattern, replacement in profanity_patterns.items():\n",
    "        if re.fullmatch(pattern, lower_word):\n",
    "            return replacement\n",
    "    replacements = {\n",
    "        r'[@4]': 'a',\n",
    "        r'[$5]': 's',\n",
    "        r'[1!|i]': 'i',\n",
    "        r'[0°]': 'o',\n",
    "        r'[3]': 'e',\n",
    "        r'[7]': 't',\n",
    "        r'[8]': 'b',\n",
    "        r'[9]': 'g',\n",
    "    }\n",
    "    for pattern, replacement in replacements.items():\n",
    "        lower_word = re.sub(pattern, replacement, lower_word)\n",
    "    return lower_word\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    \"\"\"Improved tokenizer that handles contractions and obfuscations\"\"\"\n",
    "    text = re.sub(r\"(\\w+)'(\\w+)\", r\"\\1'\\2\", text)\n",
    "    return re.findall(r\"[@\\w$!']+\", text.lower())\n",
    "\n",
    "def reduce_redundant_letters(word):\n",
    "    \"\"\"Handle excessive letter repetition\"\"\"\n",
    "    reduced = re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "    if reduced not in english_words:\n",
    "        reduced = re.sub(r'(.)\\1+', r'\\1', reduced)\n",
    "    return reduced\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"Normalize word with profanity detection focus\"\"\"\n",
    "    if len(word) <= 2 or word in STOP_WORDS:\n",
    "        return word\n",
    "    deobfuscated = deobfuscate_word(word)\n",
    "    if deobfuscated.endswith('ing'):\n",
    "        base_form = deobfuscated[:-3]\n",
    "        if base_form in profanity_dict:\n",
    "            return base_form\n",
    "        if f\"{base_form}in\" in profanity_dict:\n",
    "            return f\"{base_form}in\"\n",
    "    if deobfuscated in profanity_dict:\n",
    "        return deobfuscated\n",
    "    closest = process.extractOne(deobfuscated, profanity_dict.keys(), score_cutoff=85)\n",
    "    if closest:\n",
    "        return closest[0]\n",
    "    return deobfuscated\n",
    "\n",
    "# Context-aware model setup using Toxic-BERT\n",
    "CONTEXT_MODEL_NAME = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONTEXT_MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CONTEXT_MODEL_NAME)\n",
    "\n",
    "def analyze_context(text):\n",
    "    \"\"\"Use transformer model to detect toxic context\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    scores = softmax(outputs.logits.detach().numpy()[0])\n",
    "    # Adjusted indices: model returns 6 scores, indices 0-5.\n",
    "    return {\n",
    "        \"toxic\": scores[0],\n",
    "        \"severe_toxic\": scores[1],\n",
    "        \"obscene\": scores[2],\n",
    "        \"threat\": scores[3],\n",
    "        \"insult\": scores[4],\n",
    "        \"identity_hate\": scores[5]\n",
    "    }\n",
    "\n",
    "def contains_profanity(text):\n",
    "    \"\"\"Enhanced profanity detection with transformer-based context awareness\"\"\"\n",
    "    words = custom_tokenize(text)\n",
    "    flagged_words = {}\n",
    "\n",
    "    # Check single words using dictionary/fuzzy logic\n",
    "    for word in words:\n",
    "        if word in STOP_WORDS or len(word) <= 2:\n",
    "            continue\n",
    "        normalized = normalize_word(word)\n",
    "        if normalized in STOP_WORDS:\n",
    "            continue\n",
    "        if normalized in profanity_dict:\n",
    "            flagged_words[word] = profanity_dict[normalized]\n",
    "\n",
    "    # Check multi-word phrases (2-4 words)\n",
    "    for phrase_length in range(2, min(5, len(words) + 1)):\n",
    "        for i in range(len(words) - phrase_length + 1):\n",
    "            phrase = ' '.join(words[i:i+phrase_length])\n",
    "            if phrase in profanity_phrases[phrase_length]:\n",
    "                for term, category in profanity_phrases[phrase_length]:\n",
    "                    if term == phrase:\n",
    "                        flagged_words[phrase] = category\n",
    "                        break\n",
    "\n",
    "    # If no profanity flagged by the dictionary logic, return early\n",
    "    if not flagged_words:\n",
    "        return \"Clean\"\n",
    "\n",
    "    # Run transformer-based context analysis on the original text\n",
    "    try:\n",
    "        toxicity_scores = analyze_context(text)\n",
    "        max_toxicity = max(toxicity_scores.values())\n",
    "        # Define thresholds (adjust these as needed)\n",
    "        if max_toxicity < 0.7:\n",
    "            return \"Clean\"\n",
    "        elif max_toxicity < 0.4:\n",
    "            return \"Clean\"\n",
    "        # Special-case: ignore \"shit\" if overall toxicity is low and certain context words exist\n",
    "        if \"shit\" in flagged_words and toxicity_scores['toxic'] < 0.5:\n",
    "            if any(word in text.lower() for word in [\"deep\", \"happening\", \"going on\"]):\n",
    "                return \"Clean\"\n",
    "    except Exception as e:\n",
    "        print(f\"Context analysis failed: {str(e)}\")\n",
    "        return flagged_words\n",
    "\n",
    "    return flagged_words\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_cases = [\n",
    "      \"You are a f@cking id!0t\",\n",
    "      \"h3ll yeah! that's $tupid\",\n",
    "      \"Sh1t, you're an @ssh0le!\",\n",
    "      \"fucking awesome\",\n",
    "      \"This is a clean sentence\",\n",
    "      \"I just shit my pants\",\n",
    "      \"Go to hell you motherfucker\",\n",
    "      \"That's some bullshit right there\",\n",
    "      \"What the f*** is this?\",\n",
    "      \"You're a dumb@ss\"\n",
    "    ]\n",
    "    for text in test_cases:\n",
    "        print(f\"Text: {text}\")\n",
    "        result = contains_profanity(text)\n",
    "        print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: You are a f@cking id!0t  \n",
    "Result: {'f@cking': 'Explicit', 'id!0t': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: h3ll yeah! that's $tupid  \n",
    "Result: {'$tupid': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: Sh1t, you're an @ssh0le!  \n",
    "Result: {'sh1t': 'Insults & Personal Attacks', '@ssh0le!': 'Explicit'}  \n",
    "\n",
    "Text: fucking awesome  \n",
    "Result: Clean  \n",
    "\n",
    "Text: This is a clean sentence  \n",
    "Result: Clean  \n",
    "\n",
    "Text: I just shit my pants  \n",
    "Result: {'shit': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: Go to hell you motherfucker  \n",
    "Result: {'motherfucker': 'Explicit'}  \n",
    "\n",
    "Text: That's some bullshit right there  \n",
    "Result: Clean  \n",
    "\n",
    "Text: What the f*** is this?  \n",
    "Result: Clean  \n",
    "\n",
    "Text: You're a dumb@ss  \n",
    "Result: {'dumb@ss': 'Insults & Personal Attacks'}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from rapidfuzz import process\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize NLP tools\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Load dictionaries\n",
    "def load_dictionary(dictionary_path):\n",
    "    \"\"\"Load English dictionary words\"\"\"\n",
    "    df = pd.read_csv(dictionary_path, header=None)\n",
    "    return set(df[0].astype(str).str.lower())\n",
    "\n",
    "def load_profanity_words(csv_file):\n",
    "    \"\"\"Load and preprocess profanity dictionary\"\"\"\n",
    "    profanity_df = pd.read_csv(csv_file, encoding=\"latin-1\")\n",
    "    profanity_df[\"Text\"] = profanity_df[\"Text\"].str.replace(r\"[\\x92\\x91\\x96\\x97]\", \"'\", regex=True)\n",
    "    single_words = {}\n",
    "    phrases = defaultdict(list)\n",
    "    for _, row in profanity_df.iterrows():\n",
    "        text = row[\"Text\"].lower()\n",
    "        if ' ' in text:  # It's a phrase\n",
    "            phrases[len(text.split())].append((text, row[\"Category\"]))\n",
    "        else:\n",
    "            single_words[text] = row[\"Category\"]\n",
    "    return single_words, phrases\n",
    "\n",
    "# Load dictionaries\n",
    "dictionary_path = \"english_dictionary.csv\"\n",
    "profanity_path = \"semantically_labeled_profanities.csv\"\n",
    "english_words = load_dictionary(dictionary_path)\n",
    "profanity_dict, profanity_phrases = load_profanity_words(profanity_path)\n",
    "\n",
    "def deobfuscate_word(word):\n",
    "    \"\"\"Convert leetspeak and symbols back to letters with profanity priority\"\"\"\n",
    "\n",
    "    lower_word = word.lower()\n",
    "    # Special-case: if the word is like f*** (an \"f\" followed by at least two asterisks)\n",
    "    if re.fullmatch(r\"f\\*{2,}\", lower_word):\n",
    "        return \"fuck\"\n",
    "    profanity_patterns = {\n",
    "        r'f[!@#$%^&*.]*u[!@#$%^&*.]*c[!@#$%^&*.]*k': 'fuck',\n",
    "        r's[!@#$%^&*.]*h[!@#$%^&*.]*i[!@#$%^&*.]*t': 'shit',\n",
    "        r'a[!@#$%^&*.]*s[!@#$%^&*.]*s': 'ass',\n",
    "        r'b[!@#$%^&*.]*i[!@#$%^&*.]*t[!@#$%^&*.]*c[!@#$%^&*.]*h': 'bitch',\n",
    "        r'c[!@#$%^&*.]*u[!@#$%^&*.]*n[!@#$%^&*.]*t': 'cunt',\n",
    "        r'd[!@#$%^&*.]*i[!@#$%^&*.]*c[!@#$%^&*.]*k': 'dick',\n",
    "        r'p[!@#$%^&*.]*u[!@#$%^&*.]*s[!@#$%^&*.]*s[!@#$%^&*.]*y': 'pussy',\n",
    "    }\n",
    "    lower_word = word.lower()\n",
    "    for pattern, replacement in profanity_patterns.items():\n",
    "        if re.fullmatch(pattern, lower_word):\n",
    "            return replacement\n",
    "    replacements = {\n",
    "        r'[@4]': 'a',\n",
    "        r'[$5]': 's',\n",
    "        r'[1!|i]': 'i',\n",
    "        r'[0°]': 'o',\n",
    "        r'[3]': 'e',\n",
    "        r'[7]': 't',\n",
    "        r'[8]': 'b',\n",
    "        r'[9]': 'g',\n",
    "    }\n",
    "    for pattern, replacement in replacements.items():\n",
    "        lower_word = re.sub(pattern, replacement, lower_word)\n",
    "    return lower_word\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    \"\"\"Improved tokenizer that handles contractions and obfuscations\"\"\"\n",
    "    text = re.sub(r\"(\\w+)'(\\w+)\", r\"\\1'\\2\", text)\n",
    "    return re.findall(r\"[@\\w$!']+\", text.lower())\n",
    "\n",
    "def reduce_redundant_letters(word):\n",
    "    \"\"\"Handle excessive letter repetition\"\"\"\n",
    "    reduced = re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "    if reduced not in english_words:\n",
    "        reduced = re.sub(r'(.)\\1+', r'\\1', reduced)\n",
    "    return reduced\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"Normalize word with profanity detection focus\"\"\"\n",
    "    if len(word) <= 2 or word in STOP_WORDS:\n",
    "        return word\n",
    "    deobfuscated = deobfuscate_word(word)\n",
    "    if deobfuscated.endswith('ing'):\n",
    "        base_form = deobfuscated[:-3]\n",
    "        if base_form in profanity_dict:\n",
    "            return base_form\n",
    "        if f\"{base_form}in\" in profanity_dict:\n",
    "            return f\"{base_form}in\"\n",
    "    if deobfuscated in profanity_dict:\n",
    "        return deobfuscated\n",
    "    closest = process.extractOne(deobfuscated, profanity_dict.keys(), score_cutoff=85)\n",
    "    if closest:\n",
    "        return closest[0]\n",
    "    return deobfuscated\n",
    "\n",
    "# Context-aware model setup using Toxic-BERT\n",
    "CONTEXT_MODEL_NAME = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONTEXT_MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CONTEXT_MODEL_NAME)\n",
    "\n",
    "def analyze_context(text):\n",
    "    \"\"\"Use transformer model to detect toxic context\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    scores = softmax(outputs.logits.detach().numpy()[0])\n",
    "    # Adjusted indices: model returns 6 scores, indices 0-5.\n",
    "    return {\n",
    "        \"toxic\": scores[0],\n",
    "        \"severe_toxic\": scores[1],\n",
    "        \"obscene\": scores[2],\n",
    "        \"threat\": scores[3],\n",
    "        \"insult\": scores[4],\n",
    "        \"identity_hate\": scores[5]\n",
    "    }\n",
    "\n",
    "def contains_profanity(text):\n",
    "    \"\"\"Enhanced profanity detection with transformer-based context awareness and special-case filtering\"\"\"\n",
    "    words = custom_tokenize(text)\n",
    "    flagged_words = {}\n",
    "\n",
    "    # Check single words using dictionary/fuzzy logic\n",
    "    for word in words:\n",
    "        if word in STOP_WORDS or len(word) <= 2:\n",
    "            continue\n",
    "        normalized = normalize_word(word)\n",
    "        if normalized in STOP_WORDS:\n",
    "            continue\n",
    "        if normalized in profanity_dict:\n",
    "            flagged_words[word] = profanity_dict[normalized]\n",
    "\n",
    "    # Check multi-word phrases (2-4 words)\n",
    "    for phrase_length in range(2, min(5, len(words) + 1)):\n",
    "        for i in range(len(words) - phrase_length + 1):\n",
    "            phrase = ' '.join(words[i:i+phrase_length])\n",
    "            if phrase in profanity_phrases[phrase_length]:\n",
    "                for term, category in profanity_phrases[phrase_length]:\n",
    "                    if term == phrase:\n",
    "                        flagged_words[phrase] = category\n",
    "                        break\n",
    "\n",
    "    # If no profanity flagged by the dictionary logic, return early\n",
    "    if not flagged_words:\n",
    "        return \"Clean\"\n",
    "\n",
    "    # Run transformer-based context analysis on the original text\n",
    "    try:\n",
    "        toxicity_scores = analyze_context(text)\n",
    "        max_toxicity = max(toxicity_scores.values())\n",
    "        # Define thresholds (adjust as needed)\n",
    "        if max_toxicity < 0.7 or max_toxicity < 0.4:\n",
    "            return \"Clean\"\n",
    "        # Special-case: ignore \"shit\" if overall toxicity is low and context words exist\n",
    "        if \"shit\" in flagged_words:\n",
    "            # Example check: if the text contains \"pants\", we assume it's literal\n",
    "            if \"pants\" in text.lower() or \"poop\" in text.lower():\n",
    "                flagged_words.pop(\"shit\")\n",
    "    except Exception as e:\n",
    "        print(f\"Context analysis failed: {str(e)}\")\n",
    "        return flagged_words\n",
    "\n",
    "    return flagged_words if flagged_words else \"Clean\"\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_cases = [\n",
    "      \"You are a f@cking id!0t\",\n",
    "      \"h3ll yeah! that's $tupid\",\n",
    "      \"Sh1t, you're an @ssh0le!\",\n",
    "      \"fucking awesome\",\n",
    "      \"This is a clean sentence\",\n",
    "      \"I just shit my pants\",\n",
    "      \"Go to hell you motherfucker\",\n",
    "      \"f*** you\",\n",
    "      \"That's some bullshit right there\",\n",
    "      \"What the f*** is this?\",\n",
    "      \"You're a dumb@ss\"\n",
    "    ]\n",
    "    for text in test_cases:\n",
    "        print(f\"Text: {text}\")\n",
    "        result = contains_profanity(text)\n",
    "        print(f\"Result: {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: You are a f@cking id!0t  \n",
    "Result: {'f@cking': 'Explicit', 'id!0t': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: h3ll yeah! that's $tupid  \n",
    "Result: {'$tupid': 'Insults & Personal Attacks'}  \n",
    "\n",
    "Text: Sh1t, you're an @ssh0le!  \n",
    "Result: {'sh1t': 'Insults & Personal Attacks', '@ssh0le!': 'Explicit'}  \n",
    "\n",
    "Text: fucking awesome  \n",
    "Result: Clean  \n",
    "\n",
    "Text: This is a clean sentence  \n",
    "Result: Clean  \n",
    "\n",
    "Text: I just shit my pants  \n",
    "Result: Clean  \n",
    "\n",
    "Text: Go to hell you motherfucker  \n",
    "Result: {'motherfucker': 'Explicit'}  \n",
    "\n",
    "Text: f*** you  \n",
    "Result: Clean  \n",
    "\n",
    "Text: That's some bullshit right there  \n",
    "Result: Clean  \n",
    "\n",
    "Text: What the f*** is this?  \n",
    "Result: Clean  \n",
    "\n",
    "Text: You're a dumb@ss  \n",
    "Result: {'dumb@ss': 'Insults & Personal Attacks'}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
